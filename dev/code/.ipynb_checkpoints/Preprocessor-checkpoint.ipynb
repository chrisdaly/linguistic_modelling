{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessor \n",
    "\n",
    "Input : \n",
    "    strings\n",
    "    \n",
    "Output:\n",
    "    list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import regex\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from typing import Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_english = \"Officers had been directing him and others to go home under the government's guidance to stay indoors. Blackburn magistrates jailed him for 26 weeks yesterday for the threats and other offences.\"\n",
    "text_spanish = \"En Italia, la nación europea más afectada hasta el momento, el número de muertos por coronavirus se ha disparado en los últimos días. Hasta el domingo, las autoridades locales habían reportado 5.476 víctimas fatales, una cifra que lo coloca por encima de China, donde surgió la epidemia a finales de 2019.\"\n",
    "text_hindi = \"दुनिया का पहला टेस्ट ट्यूब बेबी 1978 में पैदा हुआ था. उसके बाद से अब तक क़रीब 80 लाख बच्चे इस तकनीक के ज़रिए दुनिया में आ चुके हैं. रिसर्चरों का मानना है कि भविष्य में इस तरीक़े से पैदा हुए बच्चों की तादाद में भारी इज़ाफ़ा देखने को मिलेगा. लेखक हेनरी टी ग्रीली का कहना है कि आने वाले समय में 20 से 40 साल की उम्र वाले सेहतमंद जोड़े लैब में गर्भ धारण कराना पसंद करेंगे. वो सेक्स बच्चा पैदा करने के लिए नहीं बल्कि ज़िस्मानी ज़रूरत और ख़ुशी के लिए करेंगे.\"\n",
    "text_arabic = 'فقال فيصل: \"قالها الأمير نايف الله يرحمه، وللأسف طلع الاستهداف منا وفينا عكس ما كنا نتوقع نسأل الله الثبات على دينه وأن يرحمنا برحمته\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT_ALPHABETIC = re.compile(r\"(((?![\\d])\\w)+)\")\n",
    "PAT_HINDI = regex.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "PAT_ARABIC = re.compile(r\"\\W+\")\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self, language: str, deaccent=True, lower=True, min_len=2, max_len=30) -> None:\n",
    "        self.language = language\n",
    "        self.deaccent = deaccent\n",
    "        self.lower = lower\n",
    "        self.min_len = min_len\n",
    "        self.max_len = max_len\n",
    "        self.stopwords = self._load_stopwords()\n",
    "        self.tokenizer = Tokenizer(self.language)\n",
    "   \n",
    "    def preprocess(self, text: str) -> list:\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "            \n",
    "        if self.deaccent:\n",
    "            text = Preprocessor.strip_accents(text)\n",
    "            \n",
    "        text = Preprocessor.clean_text(text)\n",
    "        tokens = self._tokenize(text)\n",
    "        tokens = [t for t in tokens if self._valid_token(t) and self._not_stop_word(t)]        \n",
    "        return tokens\n",
    "    \n",
    "    def _tokenize(self, text: str) -> list: \n",
    "        tokenized = self.tokenizer.tokenize(text)\n",
    "        return tokenized\n",
    "    \n",
    "    def _load_stopwords(self) -> list:\n",
    "        stopwords = []\n",
    "        try:\n",
    "            nltk_stopwords = nltk.corpus.stopwords.words(self.language)\n",
    "            stopwords.extend(nltk_stopwords)\n",
    "\n",
    "            # During the clean process, deaccenting happens before removing stopwords => need to add extra unaccented stopwords.\n",
    "            if self.deaccent == True:\n",
    "                stopwords_deaccented = [Preprocessor.strip_accents(t) for t in nltk_stopwords]\n",
    "                stopwords.extend(stopwords_deaccented)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Couldn't find any NLTK stopwords for {self.language}\")\n",
    "\n",
    "        finally:\n",
    "            return list(set(stopwords))\n",
    "        \n",
    "    def _not_stop_word(self, token: str) -> bool: \n",
    "        return token not in self.stopwords\n",
    "\n",
    "    def _valid_token(self, token: str) -> bool:\n",
    "        return all([len(token) > self.min_len, len(token) < self.max_len, not token.startswith(\"http\")])\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        # Ignore any control or punctuation characters.                              \n",
    "        cleaned = \"\".join(c for c in text if unicodedata.category(c)[0] not in [\"C\", \"P\"])\n",
    "        return cleaned\n",
    "    \n",
    "    @staticmethod\n",
    "    def strip_accents(text):\n",
    "        stripped = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "        return stripped\n",
    "                                              \n",
    "class Tokenizer:\n",
    "    def __init__(self, language):\n",
    "        self.language = language\n",
    "\n",
    "    @property\n",
    "    def token_pattern(self) -> Pattern:\n",
    "        patterns = {\"english\": PAT_ALPHABETIC, \"hindi\": PAT_HINDI, \"arabic\": PAT_HINDI}\n",
    "        token_pattern = patterns.get(self.language, PAT_ALPHABETIC)\n",
    "        print(f\"{token_pattern}\")\n",
    "        \n",
    "        return token_pattern\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Return a function that split a string in sequence of tokens\"\"\"    \n",
    "        for match in self.token_pattern.finditer(text):\n",
    "            yield match.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('(((?![\\\\d])\\\\w)+)')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['officers',\n",
       " 'directing',\n",
       " 'others',\n",
       " 'home',\n",
       " 'governments',\n",
       " 'guidance',\n",
       " 'stay',\n",
       " 'indoors',\n",
       " 'blackburn',\n",
       " 'magistrates',\n",
       " 'jailed',\n",
       " 'weeks',\n",
       " 'yesterday',\n",
       " 'threats',\n",
       " 'offences']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\"english\")\n",
    "list(preprocessor.preprocess(text_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex.Regex('(?u)\\\\b\\\\w\\\\w+\\\\b', flags=regex.V0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['दनिया',\n",
       " 'पहला',\n",
       " 'टसट',\n",
       " 'टयब',\n",
       " 'बबी',\n",
       " '1978',\n",
       " 'पदा',\n",
       " 'करीब',\n",
       " 'लाख',\n",
       " 'तकनीक',\n",
       " 'जरिए',\n",
       " 'दनिया',\n",
       " 'रिसरचरो',\n",
       " 'मानना',\n",
       " 'भविषय',\n",
       " 'तरीक',\n",
       " 'पदा',\n",
       " 'बचचो',\n",
       " 'तादाद',\n",
       " 'भारी',\n",
       " 'इजाफा',\n",
       " 'दखन',\n",
       " 'मिलगा',\n",
       " 'लखक',\n",
       " 'हनरी',\n",
       " 'गरीली',\n",
       " 'कहना',\n",
       " 'साल',\n",
       " 'उमर',\n",
       " 'सहतमद',\n",
       " 'जोड',\n",
       " 'गरभ',\n",
       " 'धारण',\n",
       " 'कराना',\n",
       " 'पसद',\n",
       " 'सकस',\n",
       " 'बचचा',\n",
       " 'पदा',\n",
       " 'बलकि',\n",
       " 'जिसमानी',\n",
       " 'जररत',\n",
       " 'खशी']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\"hindi\")\n",
    "list(preprocessor.preprocess(text_hindi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex.Regex('(?u)\\\\b\\\\w\\\\w+\\\\b', flags=regex.V0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['فقال',\n",
       " 'فيصل',\n",
       " 'قالها',\n",
       " 'الامير',\n",
       " 'نايف',\n",
       " 'الله',\n",
       " 'يرحمه',\n",
       " 'وللاسف',\n",
       " 'طلع',\n",
       " 'الاستهداف',\n",
       " 'منا',\n",
       " 'وفينا',\n",
       " 'عكس',\n",
       " 'كنا',\n",
       " 'نتوقع',\n",
       " 'نسال',\n",
       " 'الله',\n",
       " 'الثبات',\n",
       " 'دينه',\n",
       " 'يرحمنا',\n",
       " 'برحمته']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\"arabic\")\n",
    "list(preprocessor.preprocess(text_arabic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('(((?![\\\\d])\\\\w)+)')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['italia',\n",
       " 'nacion',\n",
       " 'europea',\n",
       " 'afectada',\n",
       " 'momento',\n",
       " 'numero',\n",
       " 'muertos',\n",
       " 'coronavirus',\n",
       " 'disparado',\n",
       " 'ultimos',\n",
       " 'dias',\n",
       " 'domingo',\n",
       " 'autoridades',\n",
       " 'locales',\n",
       " 'reportado',\n",
       " 'victimas',\n",
       " 'fatales',\n",
       " 'cifra',\n",
       " 'coloca',\n",
       " 'encima',\n",
       " 'china',\n",
       " 'surgio',\n",
       " 'epidemia',\n",
       " 'finales']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = Preprocessor(\"spanish\")\n",
    "list(preprocessor.preprocess(text_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
